{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "642 images loaded\n"
     ]
    }
   ],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "mypath = \".//dataset4//images//\"\n",
    "\n",
    "\n",
    "file_names = [f for f in listdir(mypath) if isfile(join(mypath, f))]\n",
    "\n",
    "print(str(len(file_names)) + ' images loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dir(directory):\n",
    "        if os.path.exists(directory):\n",
    "            shutil.rmtree(directory)\n",
    "        os.makedirs(directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and Test Data Extraction Complete\n"
     ]
    }
   ],
   "source": [
    "green_count = 0\n",
    "red_count = 0\n",
    "others_count=0\n",
    "training_size = 100\n",
    "test_size =80\n",
    "training_images = []\n",
    "training_labels = []\n",
    "test_images = []\n",
    "test_labels = []\n",
    "size=1\n",
    "green_dir_train = \"./datasets4/images/train/green/\"\n",
    "red_dir_train = \"./datasets4/images/train/red/\"\n",
    "others_dir_train = \"./datasets4/images/train/others/\"\n",
    "green_dir_val = \"./datasets4/images/validation/green/\"\n",
    "red_dir_val = \"./datasets4/images/validation/red/\"\n",
    "others_dir_val = \"./datasets4/images/validation/others/\"\n",
    "\n",
    "\n",
    "\n",
    "make_dir(green_dir_train)\n",
    "make_dir(red_dir_train)\n",
    "make_dir(others_dir_train)\n",
    "make_dir(green_dir_val)\n",
    "make_dir(red_dir_val)\n",
    "make_dir(others_dir_val)\n",
    "\n",
    "def getZeros(number): #for counting images\n",
    "    if(number > 10 and number < 100):\n",
    "        return \"0\"\n",
    "    if(number < 10):\n",
    "        return \"00\"\n",
    "    else:\n",
    "        return \"\"\n",
    "\n",
    "for i, file in enumerate(file_names):\n",
    "    \n",
    "    if file_names[i][0] == \"g\":\n",
    "        if green_count>=training_size+test_size:\n",
    "            continue\n",
    "        else:\n",
    "            #print(file,end=',')\n",
    "            image = cv2.imread(mypath+file)\n",
    "            image = cv2.resize(image,(size, size), interpolation = cv2.INTER_AREA)\n",
    "            if green_count < training_size:\n",
    "                training_images.append(image)\n",
    "                training_labels.append(0)\n",
    "                zeros = getZeros(green_count)\n",
    "                cv2.imwrite(green_dir_train + \"green\" + str(zeros) + str(green_count) + \".jpg\", image)\n",
    "            if green_count >= training_size and green_count <= training_size+test_size:\n",
    "                test_images.append(image)\n",
    "                test_labels.append(0)\n",
    "                zeros = getZeros(green_count-1000)\n",
    "                cv2.imwrite(green_dir_val + \"green\" + str(zeros) + str(green_count-1000) + \".jpg\", image)\n",
    "        green_count += 1\n",
    "        \n",
    "    if file_names[i][0] == \"r\":\n",
    "        if red_count >= training_size+test_size:\n",
    "            continue\n",
    "        else:\n",
    "            #print(file,end=',')\n",
    "            image = cv2.imread(mypath+file)\n",
    "            image = cv2.resize(image, (size,size), interpolation = cv2.INTER_AREA)\n",
    "            if red_count < training_size:\n",
    "                training_images.append(image)\n",
    "                training_labels.append(1)\n",
    "                zeros = getZeros(red_count)\n",
    "                cv2.imwrite(red_dir_train + \"red\" + str(zeros) + str(red_count) + \".jpg\", image)\n",
    "            if red_count >= training_size and red_count <= training_size+test_size:\n",
    "                test_images.append(image)\n",
    "                test_labels.append(1)\n",
    "                zeros = getZeros(red_count-1000)\n",
    "                cv2.imwrite(red_dir_val + \"red\" + str(zeros) + str(red_count-1000) + \".jpg\", image)\n",
    "        red_count += 1\n",
    "        \n",
    "   \n",
    "\n",
    "\n",
    "    if file_names[i][0] == \"o\":\n",
    "        if others_count>=training_size+test_size:\n",
    "            continue\n",
    "        else:\n",
    "            #print(file,end=',')\n",
    "            image = cv2.imread(mypath+file)\n",
    "            image = cv2.resize(image,(size, size), interpolation = cv2.INTER_AREA)\n",
    "            if others_count < training_size:\n",
    "                training_images.append(image)\n",
    "                training_labels.append(2)\n",
    "                zeros = getZeros(others_count)\n",
    "                cv2.imwrite(others_dir_train + \"others\" + str(zeros) + str(others_count) + \".jpg\", image)\n",
    "            if others_count >= training_size and others_count <= training_size+test_size:\n",
    "                test_images.append(image)\n",
    "                test_labels.append(2)\n",
    "                zeros = getZeros(others_count-1000)\n",
    "                cv2.imwrite(others_dir_val + \"others\" + str(zeros) + str(others_count-1000) + \".jpg\", image)\n",
    "        others_count += 1\n",
    "        \n",
    "    \n",
    "        \n",
    "        \n",
    "    if green_count == training_size+test_size+1 and red_count == training_size+test_size+1 and others_count==training_size+test_size+1:\n",
    "        break\n",
    "\n",
    "print(\"Training and Test Data Extraction Complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez('colour_blind_training_data.npz', np.array(training_images))\n",
    "np.savez('colour_blind_training_labels.npz', np.array(training_labels))\n",
    "np.savez('colour_blind_test_data.npz', np.array(test_images))\n",
    "np.savez('colour_blind_test_labels.npz', np.array(test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def load_data_training_and_test(datasetname):\n",
    "    \n",
    "    npzfile = np.load(datasetname + \"_training_data.npz\")\n",
    "    train = npzfile['arr_0']\n",
    "    \n",
    "    npzfile = np.load(datasetname + \"_training_labels.npz\")\n",
    "    train_labels = npzfile['arr_0']\n",
    "    \n",
    "    npzfile = np.load(datasetname + \"_test_data.npz\")\n",
    "    test = npzfile['arr_0']\n",
    "    \n",
    "    npzfile = np.load(datasetname + \"_test_labels.npz\")\n",
    "    test_labels = npzfile['arr_0']\n",
    "\n",
    "    return (train, train_labels), (test, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for i in range(1,11):\n",
    "    random = np.random.randint(0, len(training_images))\n",
    "    cv2.imshow(\"image_\"+str(i), training_images[random])\n",
    "    if training_labels[random] == 0:\n",
    "        print(str(i) + \" - green\")\n",
    "    if training_labels[random] == 1:\n",
    "        print(str(i) + \" - red\")\n",
    "    else:\n",
    "        print(str(i) + \" - others\")\n",
    "    \n",
    "    cv2.waitKey(0)\n",
    "    \n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300, 1, 1, 3)\n",
      "(300, 3)\n",
      "(240, 1, 1, 3)\n",
      "(240, 3)\n"
     ]
    }
   ],
   "source": [
    "(x_train, y_train), (x_test, y_test) = load_data_training_and_test(\"colour_blind\")\n",
    "\n",
    "# Reshaping our label data from (2000,) to (2000,1) and test data from (1000,) to (1000,1)\n",
    "#y_train = y_train.reshape(y_train.shape[0], 1)\n",
    "#y_test = y_test.reshape(y_test.shape[0], 1)\n",
    "y_train=np_utils.to_categorical(y_train)\n",
    "y_test=np_utils.to_categorical(y_test)\n",
    "# Change our image type to float32 data type\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "\n",
    "# Normalize our data by changing the range from (0 to 255) to (0 to 1)\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 1.], dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 1, 1, 32)          128       \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 1, 1, 64)          2112      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 1, 1, 64)          0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 1, 1, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               8320      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 3)                 387       \n",
      "=================================================================\n",
      "Total params: 10,947\n",
      "Trainable params: 10,947\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from keras.optimizers import SGD\n",
    "\n",
    "batch_size = 16\n",
    "epochs = 15\n",
    "\n",
    "img_rows = x_train[0].shape[0]\n",
    "img_cols = x_train[0].shape[1]\n",
    "input_shape = (img_rows, img_cols, 3)\n",
    "model=Sequential() #use to make layers\n",
    "model.add(Conv2D(32,kernel_size=(1,1),activation='relu',input_shape=input_shape))\n",
    "model.add(Conv2D(64,(1,1),activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(1,1)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128,activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(3,activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy',optimizer=SGD(0.01),metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 300 samples, validate on 240 samples\n",
      "Epoch 1/15\n",
      "300/300 [==============================] - 1s 4ms/step - loss: 1.1023 - acc: 0.3067 - val_loss: 1.0898 - val_acc: 0.5167\n",
      "Epoch 2/15\n",
      "300/300 [==============================] - 0s 178us/step - loss: 1.0952 - acc: 0.3833 - val_loss: 1.0766 - val_acc: 0.7583\n",
      "Epoch 3/15\n",
      "300/300 [==============================] - 0s 191us/step - loss: 1.0775 - acc: 0.4800 - val_loss: 1.0646 - val_acc: 0.9500\n",
      "Epoch 4/15\n",
      "300/300 [==============================] - 0s 198us/step - loss: 1.0695 - acc: 0.6000 - val_loss: 1.0532 - val_acc: 0.9542\n",
      "Epoch 5/15\n",
      "300/300 [==============================] - 0s 194us/step - loss: 1.0573 - acc: 0.6367 - val_loss: 1.0420 - val_acc: 0.9542\n",
      "Epoch 6/15\n",
      "300/300 [==============================] - 0s 194us/step - loss: 1.0464 - acc: 0.6867 - val_loss: 1.0302 - val_acc: 0.9542\n",
      "Epoch 7/15\n",
      "300/300 [==============================] - 0s 167us/step - loss: 1.0398 - acc: 0.7233 - val_loss: 1.0189 - val_acc: 0.9542\n",
      "Epoch 8/15\n",
      "300/300 [==============================] - 0s 206us/step - loss: 1.0276 - acc: 0.7733 - val_loss: 1.0070 - val_acc: 0.9708\n",
      "Epoch 9/15\n",
      "300/300 [==============================] - 0s 161us/step - loss: 1.0190 - acc: 0.7667 - val_loss: 0.9940 - val_acc: 0.9708\n",
      "Epoch 10/15\n",
      "300/300 [==============================] - 0s 163us/step - loss: 0.9992 - acc: 0.8467 - val_loss: 0.9794 - val_acc: 0.9708\n",
      "Epoch 11/15\n",
      "300/300 [==============================] - 0s 166us/step - loss: 0.9890 - acc: 0.8167 - val_loss: 0.9637 - val_acc: 0.9708\n",
      "Epoch 12/15\n",
      "300/300 [==============================] - 0s 168us/step - loss: 0.9746 - acc: 0.8600 - val_loss: 0.9462 - val_acc: 0.9708\n",
      "Epoch 13/15\n",
      "300/300 [==============================] - 0s 197us/step - loss: 0.9694 - acc: 0.8433 - val_loss: 0.9278 - val_acc: 0.9708\n",
      "Epoch 14/15\n",
      "300/300 [==============================] - 0s 173us/step - loss: 0.9575 - acc: 0.8433 - val_loss: 0.9090 - val_acc: 0.9708\n",
      "Epoch 15/15\n",
      "300/300 [==============================] - 0s 179us/step - loss: 0.9185 - acc: 0.9167 - val_loss: 0.8875 - val_acc: 0.9708\n",
      "240/240 [==============================] - 0s 32us/step\n",
      "test losses 0.8874944607416789\n",
      "test accuracy 0.9708333333333333\n"
     ]
    }
   ],
   "source": [
    "history=model.fit(x_train,y_train,batch_size=batch_size,epochs=epochs,validation_data=(x_test,y_test),shuffle=True)\n",
    "score=model.evaluate(x_test,y_test,verbose=1)\n",
    "print('test losses',score[0])\n",
    "print('test accuracy',score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"colour_blindness_97.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "<class 'str'>\n",
      "red\n",
      "1\n",
      "<class 'str'>\n",
      "red\n",
      "1\n",
      "<class 'str'>\n",
      "red\n",
      "0\n",
      "<class 'str'>\n",
      "green\n",
      "1\n",
      "<class 'str'>\n",
      "red\n",
      "1\n",
      "<class 'str'>\n",
      "red\n",
      "2\n",
      "<class 'str'>\n",
      "others\n",
      "1\n",
      "<class 'str'>\n",
      "red\n",
      "2\n",
      "<class 'str'>\n",
      "others\n",
      "1\n",
      "<class 'str'>\n",
      "red\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from keras.models import load_model\n",
    "\n",
    "classifier = load_model('colour_blindness_97.h5')\n",
    "\n",
    "def draw_test(name, pred, input_im):\n",
    "    print(pred)\n",
    "    BLACK = [0,0,0]\n",
    "    print(type(pred))\n",
    "    if pred == \"0\":\n",
    "        pred = \"green\"\n",
    "    if pred == \"1\":\n",
    "        pred = \"red\"\n",
    "    if pred == \"2\":\n",
    "        pred = \"others\"\n",
    "    print(pred)\n",
    "    '''   \n",
    "    expanded_image = cv2.copyMakeBorder(input_im, 0, 0, 0, imageL.shape[0] ,\n",
    "                                        cv2.BORDER_CONSTANT,value=BLACK)\n",
    "    #expanded_image = cv2.cvtColor(expanded_image, cv2.COLOR_GRAY2BGR)\n",
    "    cv2.putText(expanded_image, str(pred), (252, 70) , \n",
    "                cv2.FONT_HERSHEY_COMPLEX_SMALL,4, (0,255,0), 2)\n",
    "    cv2.imshow(name, expanded_image)\n",
    "    '''\n",
    "\n",
    "for i in range(0,10):\n",
    "    rand = np.random.randint(0,len(x_test))\n",
    "    input_im = x_test[rand]\n",
    "\n",
    "    imageL = cv2.resize(input_im, None, fx=2, fy=2, \n",
    "                        interpolation = cv2.INTER_CUBIC)\n",
    "    cv2.imshow(\"Test Image\", imageL)\n",
    "\n",
    "    input_im = input_im.reshape(1,1,1,3) \n",
    "    \n",
    "    ## Get Prediction\n",
    "    res = str(classifier.predict_classes(input_im, 1, verbose = 0)[0])\n",
    "\n",
    "    draw_test(\"Prediction\", res,input_im) \n",
    "    cv2.waitKey(0)\n",
    "\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#yeh mera khud ka h...yeh copy ni krna\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt \n",
    "nemo = cv2.imread('image.jpg')\n",
    "plt.imshow(nemo)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(167, 302, 3)\n"
     ]
    }
   ],
   "source": [
    "#yeh mera khud ka tp h\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "x = np.random.randint(0,5,(500,500))\n",
    "img = cv2.imread('image.jpg')\n",
    "p = img.shape\n",
    "print(p)\n",
    "rows,cols,_ = img.shape\n",
    "\n",
    "for i in range(rows):\n",
    "    for j in range(cols):\n",
    "        k = img[i,j]\n",
    "        #print (k,end=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = load_model('colour_blindness_97.h5')\n",
    "def new(fr):\n",
    "    global classifier\n",
    "    res = str(classifier.predict_classes(fr, 1, verbose = 0)[0])\n",
    "    #print(res)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9179\n",
      "11025\n",
      "30230\n"
     ]
    }
   ],
   "source": [
    "import cv2 \n",
    "\n",
    "img=cv2.imread('image.jpg')\n",
    "row=img.shape[0]\n",
    "col=img.shape[1]\n",
    "red=0\n",
    "green=0\n",
    "others=0\n",
    "frame1 = img.astype('float32')\n",
    "frame1 /= 255\n",
    "for i in range(row):\n",
    "    for j in range(col):\n",
    "        a=new(frame1[i][j].reshape(1,1,1,3))\n",
    "        if(a=='0'):\n",
    "            red=red+1\n",
    "        elif(a=='1'):\n",
    "            green=green+1\n",
    "        elif(a=='2'):\n",
    "            others=others+1\n",
    "print(red)\n",
    "print(green)\n",
    "print(others)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0711 21:24:19.239754 139701457000256 deprecation_wrapper.py:119] From /home/adhoc/.local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0711 21:24:19.256706 139701457000256 deprecation_wrapper.py:119] From /home/adhoc/.local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W0711 21:24:19.297136 139701457000256 deprecation_wrapper.py:119] From /home/adhoc/.local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "W0711 21:24:19.300342 139701457000256 deprecation_wrapper.py:119] From /home/adhoc/.local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:131: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0711 21:24:19.301549 139701457000256 deprecation_wrapper.py:119] From /home/adhoc/.local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "W0711 21:24:19.315654 139701457000256 deprecation.py:506] From /home/adhoc/.local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "W0711 21:24:19.429322 139701457000256 deprecation_wrapper.py:119] From /home/adhoc/.local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "W0711 21:24:19.543002 139701457000256 deprecation_wrapper.py:119] From /home/adhoc/.local/lib/python3.7/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W0711 21:24:19.666788 139701457000256 deprecation.py:323] From /home/adhoc/.local/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 1, 1, 3)\n",
      "2\n",
      "(1, 1, 1, 3)\n",
      "2\n",
      "(1, 1, 1, 3)\n",
      "2\n",
      "(1, 1, 1, 3)\n",
      "2\n",
      "(1, 1, 1, 3)\n",
      "2\n",
      "(1, 1, 1, 3)\n",
      "2\n",
      "(1, 1, 1, 3)\n",
      "2\n",
      "(1, 1, 1, 3)\n",
      "2\n",
      "(1, 1, 1, 3)\n",
      "2\n",
      "(1, 1, 1, 3)\n",
      "2\n",
      "(1, 1, 1, 3)\n",
      "2\n",
      "(1, 1, 1, 3)\n",
      "2\n",
      "(1, 1, 1, 3)\n",
      "2\n",
      "(1, 1, 1, 3)\n",
      "2\n",
      "(1, 1, 1, 3)\n",
      "2\n",
      "(1, 1, 1, 3)\n",
      "2\n",
      "(1, 1, 1, 3)\n",
      "2\n",
      "(1, 1, 1, 3)\n",
      "2\n",
      "(1, 1, 1, 3)\n",
      "2\n",
      "(1, 1, 1, 3)\n",
      "2\n",
      "(1, 1, 1, 3)\n",
      "2\n",
      "(1, 1, 1, 3)\n",
      "2\n",
      "(1, 1, 1, 3)\n",
      "2\n",
      "(1, 1, 1, 3)\n",
      "2\n",
      "(1, 1, 1, 3)\n",
      "2\n",
      "(1, 1, 1, 3)\n",
      "2\n",
      "(1, 1, 1, 3)\n",
      "2\n",
      "(1, 1, 1, 3)\n",
      "2\n",
      "(1, 1, 1, 3)\n",
      "2\n",
      "(1, 1, 1, 3)\n",
      "2\n",
      "(1, 1, 1, 3)\n",
      "2\n",
      "(1, 1, 1, 3)\n",
      "2\n",
      "(1, 1, 1, 3)\n",
      "2\n",
      "(1, 1, 1, 3)\n",
      "2\n",
      "(1, 1, 1, 3)\n",
      "2\n",
      "(1, 1, 1, 3)\n",
      "2\n",
      "(1, 1, 1, 3)\n",
      "2\n",
      "(1, 1, 1, 3)\n",
      "2\n",
      "(1, 1, 1, 3)\n",
      "2\n",
      "(1, 1, 1, 3)\n",
      "2\n",
      "(1, 1, 1, 3)\n",
      "2\n",
      "(1, 1, 1, 3)\n",
      "2\n",
      "(1, 1, 1, 3)\n",
      "2\n",
      "(1, 1, 1, 3)\n",
      "2\n",
      "(1, 1, 1, 3)\n",
      "2\n",
      "(1, 1, 1, 3)\n",
      "2\n",
      "(1, 1, 1, 3)\n",
      "2\n",
      "(1, 1, 1, 3)\n",
      "2\n",
      "(1, 1, 1, 3)\n",
      "2\n",
      "(1, 1, 1, 3)\n",
      "2\n",
      "(1, 1, 1, 3)\n",
      "2\n",
      "(1, 1, 1, 3)\n",
      "2\n",
      "(1, 1, 1, 3)\n",
      "2\n",
      "(1, 1, 1, 3)\n",
      "2\n",
      "(1, 1, 1, 3)\n",
      "2\n",
      "(1, 1, 1, 3)\n",
      "2\n",
      "(1, 1, 1, 3)\n",
      "2\n",
      "(1, 1, 1, 3)\n",
      "2\n",
      "(1, 1, 1, 3)\n",
      "2\n",
      "(1, 1, 1, 3)\n",
      "2\n",
      "(1, 1, 1, 3)\n",
      "2\n",
      "(1, 1, 1, 3)\n",
      "2\n",
      "(1, 1, 1, 3)\n",
      "2\n",
      "(1, 1, 1, 3)\n",
      "2\n",
      "(1, 1, 1, 3)\n",
      "2\n",
      "(1, 1, 1, 3)\n",
      "2\n",
      "(1, 1, 1, 3)\n",
      "2\n",
      "(1, 1, 1, 3)\n",
      "2\n",
      "(1, 1, 1, 3)\n",
      "2\n",
      "(1, 1, 1, 3)\n",
      "2\n",
      "(1, 1, 1, 3)\n",
      "2\n",
      "(1, 1, 1, 3)\n",
      "2\n",
      "(1, 1, 1, 3)\n",
      "2\n",
      "(1, 1, 1, 3)\n",
      "2\n",
      "(1, 1, 1, 3)\n",
      "2\n",
      "(1, 1, 1, 3)\n",
      "2\n",
      "(1, 1, 1, 3)\n",
      "2\n",
      "(1, 1, 1, 3)\n",
      "2\n",
      "(1, 1, 1, 3)\n",
      "2\n",
      "(1, 1, 1, 3)\n",
      "2\n",
      "(1, 1, 1, 3)\n",
      "2\n",
      "(1, 1, 1, 3)\n",
      "2\n",
      "(1, 1, 1, 3)\n",
      "2\n",
      "(1, 1, 1, 3)\n",
      "2\n",
      "(1, 1, 1, 3)\n",
      "2\n",
      "(1, 1, 1, 3)\n",
      "2\n",
      "(1, 1, 1, 3)\n",
      "2\n",
      "(1, 1, 1, 3)\n",
      "2\n",
      "(1, 1, 1, 3)\n",
      "2\n",
      "(1, 1, 1, 3)\n",
      "2\n",
      "(1, 1, 1, 3)\n",
      "2\n",
      "(1, 1, 1, 3)\n",
      "2\n",
      "(1, 1, 1, 3)\n",
      "2\n",
      "(1, 1, 1, 3)\n",
      "2\n",
      "(1, 1, 1, 3)\n",
      "2\n",
      "(1, 1, 1, 3)\n",
      "2\n",
      "(1, 1, 1, 3)\n",
      "2\n",
      "(1, 1, 1, 3)\n",
      "2\n",
      "(1, 1, 1, 3)\n",
      "2\n",
      "(1, 1, 1, 3)\n",
      "2\n",
      "(1, 1, 1, 3)\n",
      "2\n",
      "(1, 1, 1, 3)\n",
      "2\n",
      "(1, 1, 1, 3)\n",
      "2\n",
      "(1, 1, 1, 3)\n",
      "2\n",
      "(1, 1, 1, 3)\n",
      "2\n",
      "(1, 1, 1, 3)\n",
      "2\n",
      "(1, 1, 1, 3)\n",
      "2\n",
      "(1, 1, 1, 3)\n",
      "2\n",
      "(1, 1, 1, 3)\n",
      "2\n",
      "(1, 1, 1, 3)\n",
      "2\n",
      "(1, 1, 1, 3)\n",
      "2\n",
      "(1, 1, 1, 3)\n",
      "2\n",
      "(1, 1, 1, 3)\n",
      "2\n",
      "(1, 1, 1, 3)\n",
      "2\n",
      "(1, 1, 1, 3)\n",
      "2\n",
      "(1, 1, 1, 3)\n",
      "2\n",
      "(1, 1, 1, 3)\n",
      "2\n",
      "(1, 1, 1, 3)\n",
      "2\n",
      "(1, 1, 1, 3)\n",
      "2\n",
      "(1, 1, 1, 3)\n",
      "2\n",
      "(1, 1, 1, 3)\n",
      "2\n",
      "(1, 1, 1, 3)\n",
      "2\n",
      "(1, 1, 1, 3)\n",
      "2\n",
      "(1, 1, 1, 3)\n",
      "2\n",
      "(1, 1, 1, 3)\n",
      "2\n",
      "(1, 1, 1, 3)\n",
      "2\n",
      "(1, 1, 1, 3)\n",
      "2\n",
      "(1, 1, 1, 3)\n",
      "2\n",
      "(1, 1, 1, 3)\n",
      "2\n",
      "(1, 1, 1, 3)\n",
      "2\n",
      "(1, 1, 1, 3)\n",
      "2\n",
      "(1, 1, 1, 3)\n",
      "2\n",
      "(1, 1, 1, 3)\n",
      "2\n",
      "(1, 1, 1, 3)\n",
      "2\n",
      "(1, 1, 1, 3)\n",
      "2\n",
      "(1, 1, 1, 3)\n",
      "2\n",
      "(1, 1, 1, 3)\n",
      "2\n",
      "(1, 1, 1, 3)\n",
      "2\n",
      "(1, 1, 1, 3)\n",
      "2\n",
      "(1, 1, 1, 3)\n",
      "2\n",
      "(1, 1, 1, 3)\n",
      "2\n",
      "(1, 1, 1, 3)\n",
      "2\n",
      "(1, 1, 1, 3)\n",
      "2\n",
      "(1, 1, 1, 3)\n",
      "2\n",
      "(1, 1, 1, 3)\n",
      "2\n",
      "(1, 1, 1, 3)\n",
      "2\n",
      "(1, 1, 1, 3)\n",
      "2\n",
      "(1, 1, 1, 3)\n",
      "2\n",
      "(1, 1, 1, 3)\n",
      "2\n",
      "(1, 1, 1, 3)\n",
      "2\n",
      "(1, 1, 1, 3)\n",
      "2\n",
      "(1, 1, 1, 3)\n",
      "2\n",
      "(1, 1, 1, 3)\n",
      "2\n",
      "(1, 1, 1, 3)\n",
      "2\n",
      "(1, 1, 1, 3)\n",
      "2\n",
      "(1, 1, 1, 3)\n",
      "2\n",
      "(1, 1, 1, 3)\n",
      "2\n",
      "(1, 1, 1, 3)\n",
      "2\n",
      "(1, 1, 1, 3)\n",
      "2\n",
      "(1, 1, 1, 3)\n",
      "2\n",
      "(1, 1, 1, 3)\n",
      "2\n",
      "(1, 1, 1, 3)\n",
      "2\n",
      "(1, 1, 1, 3)\n",
      "2\n",
      "(1, 1, 1, 3)\n",
      "2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<function destroyAllWindows>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cv2 \n",
    "import numpy as np\n",
    "from keras.models import load_model \n",
    "\n",
    "classifier=load_model('colour_blindness_97.h5')\n",
    "def draw_test(name, pred, input_im):\n",
    "    BLACK = [0,0,0]\n",
    "    if pred == \"[0]\":\n",
    "        pred = \"red\"\n",
    "    if pred == \"[1]\":\n",
    "        pred = \"green\"\n",
    "    if pred == \"[2]\":\n",
    "        pred==\"others\"\n",
    "    \n",
    "    \n",
    "    \n",
    "    cv2.putText(input_im, str(pred), (180, 70) , \n",
    "                cv2.FONT_HERSHEY_COMPLEX_SMALL,2, (0,255,0), 2)\n",
    "    cv2.imshow(name,input_im)\n",
    "   \n",
    "cam = cv2.VideoCapture(0)\n",
    "while cam.isOpened():\n",
    "    ret, frame= cam.read(0)\n",
    "    frames=cv2.resize(frame,(1,1),interpolation=cv2.INTER_AREA)\n",
    "    img_rows=frames[0].shape[0]\n",
    "    img_cols=frames[0].shape[1]\n",
    "    frames=frames.reshape(1,frames.shape[0],img_rows,img_cols)\n",
    "    frames=frames.astype('float32')\n",
    "    frames/=255\n",
    "\n",
    "    print(frames.shape)\n",
    "    \n",
    "    pred=str(classifier.predict_classes(frames,verbose=0)[0])\n",
    "    print(pred)\n",
    "    draw_test(\"prediction\",pred,frame)\n",
    "    if cv2.waitKey(1) == 27 :\n",
    "        break\n",
    "    \n",
    "cam.release()\n",
    "cv2.destroyAllWindows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for trial\n",
    "import cv2\n",
    "\n",
    "\n",
    "cap=cv2.VideoCapture(0)\n",
    "\n",
    "while cap.isOpened():\n",
    "    status,frame=cap.read()\n",
    "    hsvimg=cv2.cvtColor(frame,cv2.COLOR_BGR2HSV)\n",
    "   \n",
    "    imgmask=cv2.inRange(hsvimg,(40,50,50),(80,255,255))\n",
    "   \n",
    "    greenpart=cv2.bitwise_and(frame,frame,mask=imgmask)\n",
    "    redpart=cv2.bitwise_and(frame,frame,mask=imgmask)\n",
    "    cv2.imshow('original',frame)\n",
    "    \n",
    "    cv2.imshow('green',greenpart)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cv2.destroyAllWindows()\n",
    "cap.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
